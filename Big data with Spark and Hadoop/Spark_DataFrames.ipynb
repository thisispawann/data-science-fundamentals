{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://spark.apache.org/images/spark-logo.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to DataFrames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "\n",
    "A DataFrame is two-dimensional. Columns can be of different data types. DataFrames accept many data inputs including series and other DataFrames. We can pass indexes (row labels) and columns (column labels). Indexes can be numbers, dates, or strings/tuples.\n",
    "\n",
    "After completing this lab we will be able to:\n",
    "\n",
    "* Load a data file into a DataFrame\n",
    "* View the data schema of a DataFrame\n",
    "* Perform basic data manipulation [Add/drop column]\n",
    "* Rename Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Mass</th>\n",
       "      <th>Width</th>\n",
       "      <th>Height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple</td>\n",
       "      <td>granny_smith</td>\n",
       "      <td>159</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apple</td>\n",
       "      <td>golden_apple</td>\n",
       "      <td>178</td>\n",
       "      <td>8.3</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple</td>\n",
       "      <td>granny_smith</td>\n",
       "      <td>163</td>\n",
       "      <td>6.8</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strawberry</td>\n",
       "      <td>june-bearing</td>\n",
       "      <td>80</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oramge</td>\n",
       "      <td>typical</td>\n",
       "      <td>8</td>\n",
       "      <td>9.7</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name          Type  Mass  Width  Height\n",
       "0       apple  granny_smith   159    3.5     3.9\n",
       "1       apple  golden_apple   178    8.3     6.4\n",
       "2       apple  granny_smith   163    6.8     9.1\n",
       "3  strawberry  june-bearing    80    1.2     1.9\n",
       "4      oramge       typical     8    9.7     7.9"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruits = pd.read_csv('fruits.csv')\n",
    "fruits.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exercise 1 - Spark Session</h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever work with pyspark, we must have to create / start a spark session. To start a spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 - <code>Creating Spark session and context</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('console').getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2 - <code>Initializing spark session</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-1KEI208:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>console</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1adc9079b10>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2 - <code>Load the data and Spark DataFrame</code>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 - Loading data into a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read dataset with respect to spark\n",
    "df_pyspark = spark.read.csv('fruits.csv')\n",
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+-----+------+\n",
      "|       _c0|         _c1| _c2|  _c3|   _c4|\n",
      "+----------+------------+----+-----+------+\n",
      "|      Name|        Type|Mass|Width|Height|\n",
      "|     apple|granny_smith| 159|  3.5|   3.9|\n",
      "|     apple|golden_apple| 178|  8.3|   6.4|\n",
      "|     apple|granny_smith| 163|  6.8|   9.1|\n",
      "|strawberry|june-bearing|  80|  1.2|   1.9|\n",
      "|    oramge|     typical|   8|  9.7|   7.9|\n",
      "+----------+------------+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3 - <code>Reading the Spark Dataset</code>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make columns Name, Type etc as our main column not c0, c1... when we are directly reading csv file, we are getting _c0, _c1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+-----+------+\n",
      "|      Name|        Type|Mass|Width|Height|\n",
      "+----------+------------+----+-----+------+\n",
      "|     apple|granny_smith| 159|  3.5|   3.9|\n",
      "|     apple|golden_apple| 178|  8.3|   6.4|\n",
      "|     apple|granny_smith| 163|  6.8|   9.1|\n",
      "|strawberry|june-bearing|  80|  1.2|   1.9|\n",
      "|    oramge|     typical|   8|  9.7|   7.9|\n",
      "+----------+------------+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header', 'true').csv('fruits.csv').show() # .show() -> to view complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header', 'true').csv('fruits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark) # dataset is convert to pandas to sql DataFrame. If we check the type(pd.read_csv('fruits.csv')) we can get pandas.core.frame.DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Preview some Spark DataFrame records</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='apple', Type='granny_smith', Mass='159', Width='3.5', Height='3.9'),\n",
       " Row(Name='apple', Type='golden_apple', Mass='178', Width='8.3', Height='6.4'),\n",
       " Row(Name='apple', Type='granny_smith', Mass='163', Width='6.8', Height='9.1')]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Check Spark DataFrame Schema</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Mass: string (nullable = true)\n",
      " |-- Width: string (nullable = true)\n",
      " |-- Height: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printSchema; bit more info about columns. Like a pandas info() which prints the datatype of a column\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why mass, width and height showing string? It is because by default it takes all the schema to string values.\n",
    "\n",
    "So, we need to add one option i.e., inferSchema = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header', 'true').csv('fruits.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Mass: integer (nullable = true)\n",
      " |-- Width: double (nullable = true)\n",
      " |-- Height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Again check the schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can able to see their correct dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Another way to view dataset</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+-----+------+\n",
      "|      Name|        Type|Mass|Width|Height|\n",
      "+----------+------------+----+-----+------+\n",
      "|     apple|granny_smith| 159|  3.5|   3.9|\n",
      "|     apple|golden_apple| 178|  8.3|   6.4|\n",
      "|     apple|granny_smith| 163|  6.8|   9.1|\n",
      "|strawberry|june-bearing|  80|  1.2|   1.9|\n",
      "|    oramge|     typical|   8|  9.7|   7.9|\n",
      "+----------+------------+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('fruits.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Mass: integer (nullable = true)\n",
      " |-- Width: double (nullable = true)\n",
      " |-- Height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>DataFrame:</code>\n",
    "\n",
    "- It is a data structures because inside it we can perform different kinds of operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check type\n",
    "type(df_pyspark)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3 - <code>Basic Data Analysis and Manipulation</code>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 - <code>Selecting columns and Indexing<code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Type', 'Mass', 'Width', 'Height']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how we get all columns\n",
    "df_pyspark.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Select the column</code>|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+-----+------+\n",
      "|      Name|        Type|Mass|Width|Height|\n",
      "+----------+------------+----+-----+------+\n",
      "|     apple|granny_smith| 159|  3.5|   3.9|\n",
      "|     apple|golden_apple| 178|  8.3|   6.4|\n",
      "|     apple|granny_smith| 163|  6.8|   9.1|\n",
      "|strawberry|june-bearing|  80|  1.2|   1.9|\n",
      "|    oramge|     typical|   8|  9.7|   7.9|\n",
      "+----------+------------+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to viw all table\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick only Name column\n",
    "df_pyspark.select('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark.select('Name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Mass: int]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick two columns\n",
    "df_pyspark.select(['Name', 'Mass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|      Name|Mass|\n",
      "+----------+----+\n",
      "|     apple| 159|\n",
      "|     apple| 178|\n",
      "|     apple| 163|\n",
      "|strawberry|  80|\n",
      "|    oramge|   8|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view all the element of two columns\n",
    "df_pyspark.select(['Name', 'Mass']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'),\n",
       " ('Type', 'string'),\n",
       " ('Mass', 'int'),\n",
       " ('Width', 'double'),\n",
       " ('Height', 'double')]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dtypes\n",
    "df_pyspark.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking <code>Describe()</code> as Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Name: string, Type: string, Mass: string, Width: string, Height: string]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+-----------------+-----------------+-----------------+\n",
      "|summary|      Name|        Type|             Mass|            Width|           Height|\n",
      "+-------+----------+------------+-----------------+-----------------+-----------------+\n",
      "|  count|         5|           5|                5|                5|                5|\n",
      "|   mean|      null|        null|            117.6|              5.9|5.839999999999999|\n",
      "| stddev|      null|        null|72.19626029095967|3.494996423460259|2.935643030070243|\n",
      "|    min|     apple|golden_apple|                8|              1.2|              1.9|\n",
      "|    max|strawberry|     typical|              178|              9.7|              9.1|\n",
      "+-------+----------+------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columnar Operation: \n",
    "\n",
    "<code>Adding columns</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Type: string, Mass: int, Width: double, Height: double, Mass After some month: int]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.withColumn('Mass After some month', df_pyspark['Mass']+20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+-----+------+---------------------+\n",
      "|      Name|        Type|Mass|Width|Height|Mass After some month|\n",
      "+----------+------------+----+-----+------+---------------------+\n",
      "|     apple|granny_smith| 159|  3.5|   3.9|                  179|\n",
      "|     apple|golden_apple| 178|  8.3|   6.4|                  198|\n",
      "|     apple|granny_smith| 163|  6.8|   9.1|                  183|\n",
      "|strawberry|june-bearing|  80|  1.2|   1.9|                  100|\n",
      "|    oramge|     typical|   8|  9.7|   7.9|                   28|\n",
      "+----------+------------+----+-----+------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumn('Mass After some month', df_pyspark['Mass']+20).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.withColumn('Mass After some month', df_pyspark['Mass']+20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+-----+------+---------------------+\n",
      "|      Name|        Type|Mass|Width|Height|Mass After some month|\n",
      "+----------+------------+----+-----+------+---------------------+\n",
      "|     apple|granny_smith| 159|  3.5|   3.9|                  179|\n",
      "|     apple|golden_apple| 178|  8.3|   6.4|                  198|\n",
      "|     apple|granny_smith| 163|  6.8|   9.1|                  183|\n",
      "|strawberry|june-bearing|  80|  1.2|   1.9|                  100|\n",
      "|    oramge|     typical|   8|  9.7|   7.9|                   28|\n",
      "+----------+------------+----+-----+------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Drop column</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.drop('Mass After some month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+-----+------+\n",
      "|      Name|        Type|Mass|Width|Height|\n",
      "+----------+------------+----+-----+------+\n",
      "|     apple|granny_smith| 159|  3.5|   3.9|\n",
      "|     apple|golden_apple| 178|  8.3|   6.4|\n",
      "|     apple|granny_smith| 163|  6.8|   9.1|\n",
      "|strawberry|june-bearing|  80|  1.2|   1.9|\n",
      "|    oramge|     typical|   8|  9.7|   7.9|\n",
      "+----------+------------+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Rename Column</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Type: string, Weight: int, Width: double, Height: double]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.withColumnRenamed('Mass', 'Weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------+-----+------+\n",
      "|      Name|        Type|Weight|Width|Height|\n",
      "+----------+------------+------+-----+------+\n",
      "|     apple|granny_smith|   159|  3.5|   3.9|\n",
      "|     apple|golden_apple|   178|  8.3|   6.4|\n",
      "|     apple|granny_smith|   163|  6.8|   9.1|\n",
      "|strawberry|june-bearing|    80|  1.2|   1.9|\n",
      "|    oramge|     typical|     8|  9.7|   7.9|\n",
      "+----------+------------+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumnRenamed('Mass', 'Weight').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e10060ad54af1f91f5de5e4e50e0602defab201c8225bb5f2d748a943f24cad5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
