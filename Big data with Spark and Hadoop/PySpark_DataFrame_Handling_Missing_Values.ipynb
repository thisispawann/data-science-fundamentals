{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>PySpark Handling Missing Values</code>\n",
    "\n",
    "* Dropping Columns\n",
    "* Dropping Rows\n",
    "* Various Parameter in Dropping Functionalities\n",
    "* Handling Missing Values By Mean, Median and Mode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step Always: <code>Start Spark Session</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('console').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| Age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|Daniel|  32|        10| 20000|\n",
      "| Misha|  33|         5| 25000|\n",
      "|Marnus|  23|         1| 10000|\n",
      "| Steve|  34|        13| 35000|\n",
      "| David|  26|        15| 40000|\n",
      "|  null|  34|      null| 20000|\n",
      "|   Tom|null|      null| 30000|\n",
      "|  null|  40|      null|  null|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read dataset\n",
    "spark.read.csv('employee.csv', header=True, inferSchema=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('employee.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| Age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|Daniel|  32|        10| 20000|\n",
      "| Misha|  33|         5| 25000|\n",
      "|Marnus|  23|         1| 10000|\n",
      "| Steve|  34|        13| 35000|\n",
      "| David|  26|        15| 40000|\n",
      "|  null|  34|      null| 20000|\n",
      "|   Tom|null|      null| 30000|\n",
      "|  null|  40|      null|  null|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Dropping Column:</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| Age|Experience|Salary|\n",
      "+----+----------+------+\n",
      "|  32|        10| 20000|\n",
      "|  33|         5| 25000|\n",
      "|  23|         1| 10000|\n",
      "|  34|        13| 35000|\n",
      "|  26|        15| 40000|\n",
      "|  34|      null| 20000|\n",
      "|null|      null| 30000|\n",
      "|  40|      null|  null|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Dropping Null Values:</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  Name|Age|Experience|Salary|\n",
      "+------+---+----------+------+\n",
      "|Daniel| 32|        10| 20000|\n",
      "| Misha| 33|         5| 25000|\n",
      "|Marnus| 23|         1| 10000|\n",
      "| Steve| 34|        13| 35000|\n",
      "| David| 26|        15| 40000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| Age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|Daniel|  32|        10| 20000|\n",
      "| Misha|  33|         5| 25000|\n",
      "|Marnus|  23|         1| 10000|\n",
      "| Steve|  34|        13| 35000|\n",
      "| David|  26|        15| 40000|\n",
      "|  null|  34|      null| 20000|\n",
      "|   Tom|null|      null| 30000|\n",
      "|  null|  40|      null|  null|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### how == all and how == any\n",
    "df_pyspark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  Name|Age|Experience|Salary|\n",
      "+------+---+----------+------+\n",
      "|Daniel| 32|        10| 20000|\n",
      "| Misha| 33|         5| 25000|\n",
      "|Marnus| 23|         1| 10000|\n",
      "| Steve| 34|        13| 35000|\n",
      "| David| 26|        15| 40000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='any').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>'any'</code> basically remove all the rows that have null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| Age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|Daniel|  32|        10| 20000|\n",
      "| Misha|  33|         5| 25000|\n",
      "|Marnus|  23|         1| 10000|\n",
      "| Steve|  34|        13| 35000|\n",
      "| David|  26|        15| 40000|\n",
      "|  null|  34|      null| 20000|\n",
      "|   Tom|null|      null| 30000|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Threshold\n",
    "df_pyspark.na.drop(how='any', thresh=2).show() # drop all row having 2 non-null values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Thresh</code>\n",
    "\n",
    "If non NULL values of particular row or column is less than thresh value then drop that row or column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  Name|Age|Experience|Salary|\n",
      "+------+---+----------+------+\n",
      "|Daniel| 32|        10| 20000|\n",
      "| Misha| 33|         5| 25000|\n",
      "|Marnus| 23|         1| 10000|\n",
      "| Steve| 34|        13| 35000|\n",
      "| David| 26|        15| 40000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='any', thresh=3).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Subset</code>\n",
    "\n",
    "If the given subset column contains any of the null value then dop that row or column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| Age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|Daniel|  32|        10| 20000|\n",
      "| Misha|  33|         5| 25000|\n",
      "|Marnus|  23|         1| 10000|\n",
      "| Steve|  34|        13| 35000|\n",
      "| David|  26|        15| 40000|\n",
      "|   Tom|null|      null| 30000|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#subset\n",
    "df_pyspark.na.drop(how='any', subset=['Name']).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Fill the missing value</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| Age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|Daniel|  32|        10| 20000|\n",
      "| Misha|  33|         5| 25000|\n",
      "|Marnus|  23|         1| 10000|\n",
      "| Steve|  34|        13| 35000|\n",
      "| David|  26|        15| 40000|\n",
      "|  null|  34|      null| 20000|\n",
      "|   Tom|null|      null| 30000|\n",
      "|  null|  40|      null|  null|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+------+\n",
      "|          Name| Age|Experience|Salary|\n",
      "+--------------+----+----------+------+\n",
      "|        Daniel|  32|        10| 20000|\n",
      "|         Misha|  33|         5| 25000|\n",
      "|        Marnus|  23|         1| 10000|\n",
      "|         Steve|  34|        13| 35000|\n",
      "|         David|  26|        15| 40000|\n",
      "|Missing Values|  34|      null| 20000|\n",
      "|           Tom|null|      null| 30000|\n",
      "|Missing Values|  40|      null|  null|\n",
      "+--------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing Values').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| Age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|Daniel|  32|        10| 20000|\n",
      "| Misha|  33|         5| 25000|\n",
      "|Marnus|  23|         1| 10000|\n",
      "| Steve|  34|        13| 35000|\n",
      "| David|  26|        15| 40000|\n",
      "|  null|  34|      null| 20000|\n",
      "|   Tom|null|      null| 30000|\n",
      "|  null|  40|      null|  null|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing Values', ['experience', 'age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|   Daniel|  32|        10| 20000|\n",
      "|    Misha|  33|         5| 25000|\n",
      "|   Marnus|  23|         1| 10000|\n",
      "|    Steve|  34|        13| 35000|\n",
      "|    David|  26|        15| 40000|\n",
      "|NA Values|  34|      null| 20000|\n",
      "|      Tom|null|      null| 30000|\n",
      "|NA Values|  40|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('NA Values', 'Name').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing values with their mean, median and mode [<code>central tendency</code>]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is something of a more professional way to handle the missing values i.e imputing the null values with mean/median/mode depending on the domain of the dataset. Here we will be using the Imputer function from the PySpark library to use the mean/median/mode functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = ['Age', 'Experience', 'Salary'],\n",
    "    outputCols= ['{}_imputed'.format(a) for a in ['Age', 'Experience', 'Salary']]\n",
    ").setStrategy('mean')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code breakdown:** There are a lot of things going on here so let’s break it down.\n",
    "\n",
    "* First, we have called the <code>Imputer function</code> from <code>PySpark’s ml. feature</code> library.\n",
    "* Then using that Imputer object we have defined our <code>input columns</code>, as well as output columns in input columns we gave the name of the column which needs to be imputed, and the output column is the imputed one.\n",
    "* Then at the last, we <code>set the strategy</code> of imputing values (here it’s <code>mean</code>) but we can either use <code>median or mode</code>depending on the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Fit and Transform</code>\n",
    "\n",
    "Now so we have used the Imputer object to impute the mean values in the place of null values but to see the changes we need to use the <code>fit-transform method</code> simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+-----------+------------------+--------------+\n",
      "|  Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+------+----+----------+------+-----------+------------------+--------------+\n",
      "|Daniel|  32|        10| 20000|         32|                10|         20000|\n",
      "| Misha|  33|         5| 25000|         33|                 5|         25000|\n",
      "|Marnus|  23|         1| 10000|         23|                 1|         10000|\n",
      "| Steve|  34|        13| 35000|         34|                13|         35000|\n",
      "| David|  26|        15| 40000|         26|                15|         40000|\n",
      "|  null|  34|      null| 20000|         34|                 8|         20000|\n",
      "|   Tom|null|      null| 30000|         31|                 8|         30000|\n",
      "|  null|  40|      null|  null|         40|                 8|         25714|\n",
      "+------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e10060ad54af1f91f5de5e4e50e0602defab201c8225bb5f2d748a943f24cad5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
